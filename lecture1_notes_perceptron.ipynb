{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ELI5:**\n",
    "\n",
    "Think of this like tuning a musical instrument. The \"weight\" $w_{ij}$ is how the instrument is currently tuned. The equation is telling you how to adjust the tuning to get it just right. If you pluck a string and it doesn't sound right (the note you play, $a_j$, doesn't match the note you want, $y_j$), you need to tighten or loosen the string (change the weight). The amount you change it by is guided by a number called the \"learning rate\" $\\mu$, which is like instructions on how much to turn the tuning peg each time. The input $x_i$ helps you understand how important that string is to the sound you're trying to make.\n",
    "\n",
    "**ELI a New Computer Science Student:**\n",
    "\n",
    "In a neural network, the weight $w_{ij}$ is a value that determines the strength of the connection between two neurons. The equation for updating the weight is:\n",
    "\n",
    "$$ w_{ij} \\leftarrow w_{ij} - \\mu \\cdot (a_j - y_j) \\cdot x_i $$\n",
    "\n",
    "Here, $y_j$ is the target value, the value you want the network to produce, and $a_j$ is the actual output value of the neuron. The difference $a_j - y_j$ is called the error. This error is multiplied by the learning rate $\\mu$, which controls how quickly the network learns. A higher learning rate may cause the network to learn faster but might overshoot the optimal solution, while a lower learning rate may lead to more precise learning but could be slower. The term $x_i$ represents the input value associated with the weight, and the whole equation tells you how to adjust the weight to get the network's output closer to the desired target value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELI5 (Explain Like I'm 5):\n",
    "\n",
    "Think of this like tuning a musical instrument. The \"weight\" $w_{ij}$ is how the instrument is currently tuned. The equation is telling you how to adjust the tuning to get it just right. If you pluck a string and it doesn't sound right (the note you play, $a_j$, doesn't match the note you want, $y_j$), you need to tighten or loosen the string (change the weight). The amount you change it by is guided by a number called the \"learning rate\" $\\mu$, which is like instructions on how much to turn the tuning peg each time. The input $x_i$ helps you understand how important that string is to the sound you're trying to make.\n",
    "\n",
    "# ELI a New Computer Science Student:\n",
    "\n",
    "In a neural network, the weight $w_{ij}$ is a value that determines the strength of the connection between two neurons. The equation for updating the weight is:\n",
    "\n",
    "$ w_{ij} \\leftarrow w_{ij} - \\mu \\cdot (a_j - y_j) \\cdot x_i $\n",
    "\n",
    "Here, $y_j$ is the target value, the value you want the network to produce, and $a_j$ is the actual output value of the neuron. The difference $a_j - y_j$ is called the error. This error is multiplied by the learning rate $\\mu$, which controls how quickly the network learns. A higher learning rate may cause the network to learn faster but might overshoot the optimal solution, while a lower learning rate may lead to more precise learning but could be slower. The term $x_i$ represents the input value associated with the weight, and the whole equation tells you how to adjust the weight to get the network's output closer to the desired target value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some notation reminders\n",
    "\n",
    "## Notation Reminder for Machine Learning Class\n",
    "\n",
    "- $ x^{l} $: Number of features.\n",
    "- $ (x_{i}, y_{i}) $: These represent the features and the label for the \\( i \\)-th data point.\n",
    "- $ \\langle a, b \\rangle $: This denotes the dot product of vectors \\( a \\) and \\( b \\).\n",
    "\n",
    "### Additional Notations:\n",
    "\n",
    "- $ \\alpha $: Learning rate in gradient-based algorithms.\n",
    "- $ \\theta $: Parameters in a machine learning model.\n",
    "- $ J(\\theta) $: Cost or Loss function.\n",
    "- $ \\nabla $: Gradient symbol, often used to find the minimum of a function.\n",
    "- $ h(x) $: Hypothesis function that the model uses to make predictions.\n",
    "\n",
    "- Hat symbol $ \\hat{y} $ is often used to denote an estimated or predicted value. For example:\n",
    "\n",
    "     If $ y $ is the true label or output, then $ \\hat{y} $ would represent the predicted label or output given by the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Tricks Explained Like I'm 5\n",
    "\n",
    "Imagine you have a group of red and blue marbles on a table, and you want to separate them using only a straight stick. But the marbles are mixed together in such a way that you can't separate them using just a straight stick. What do you do?\n",
    "\n",
    "Think of lifting these marbles up into the air, like tossing them up onto a sheet hanging from the ceiling. Now, you can easily separate them using a stick! When you bring them back down to the table, the stick's position would look like a complex curve, but you've managed to separate the marbles!\n",
    "\n",
    "### In More Technical Terms\n",
    "\n",
    "In machine learning, this \"lifting\" is like transforming data into a higher-dimensional space. The trick is that you don't actually have to do the full computation in this higher space. Instead, you can use a function, called a \"kernel,\" to calculate how \"similar\" two points are in this higher-dimensional space without actually going there. \n",
    "\n",
    "Mathematically, this is like replacing the dot product $ \\langle x, z \\rangle $ in the original space with $ \\kappa(x, z) $ in the higher-dimensional space:\n",
    "\n",
    "$$\n",
    "\\kappa(x, z) = \\phi(x) \\cdot \\phi(z)\n",
    "$$\n",
    "\n",
    "Here, $ \\phi(x) $ and $ \\phi(z) $ are the transformations of the original vectors $ x $ and $ z $ to the higher-dimensional space. \n",
    "\n",
    "This trick allows you to use linear algorithms, like Support Vector Machines (SVMs), to solve problems that are not linearly separable in the original space.\n",
    "\n",
    "## Adding a Non-linear Feature to Solve XOR\n",
    "\n",
    "When dealing with non-linearly separable data like the XOR problem, one common technique is to introduce a non-linear feature to make it linearly separable in a higher-dimensional space.\n",
    "\n",
    "### What Does \"Adding a Non-linear Feature\" Mean?\n",
    "\n",
    "The core idea is to take the original feature space $ (x_1, x_2) $ and map it to a higher-dimensional feature space $ (x_1, x_2, z) $, where $ z $ is the new non-linear feature. By doing so, you may be able to separate the classes with a hyperplane in the higher-dimensional space even if you can't in the original space.\n",
    "\n",
    "### The Formula's Role\n",
    "\n",
    "The formula for this is:\n",
    "\n",
    "$$\n",
    "x_1 + x_2 - 2 \\times x_1 \\times x_2 - \\frac{1}{2} = 0\n",
    "$$\n",
    "\n",
    "Here, the term $ -2 \\times x_1 \\times x_2 $ acts as the non-linear feature $ z $ that helps to make the data linearly separable.\n",
    "\n",
    "For example, with this transformation:\n",
    "\n",
    "- $ (0,0) $ becomes $ (0, 0, -2 \\times 0 \\times 0) = (0, 0, 0) $\n",
    "- $ (1,0) $ becomes $ (1, 0, -2 \\times 1 \\times 0) = (1, 0, 0) $\n",
    "- $ (0,1) $ becomes $ (0, 1, -2 \\times 0 \\times 1) = (0, 1, 0) $\n",
    "- $ (1,1) $ becomes $ (1, 1, -2 \\times 1 \\times 1) = (1, 1, -2) $\n",
    "\n",
    "By moving to this higher-dimensional space, the points corresponding to an XOR output of 1 (namely $ (1,0,0) $ and $ (0,1,0) $) are separated from the points corresponding to an XOR output of 0 ($ (0,0,0) $ and $ (1,1,-2) $).\n",
    "\n",
    "Thus, the formula allows for a hyperplane that can successfully classify the transformed data points, achieving a solution to the XOR problem that a single-layer neural network could not handle.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Greek Letters \n",
    "\n",
    "### Alpha $ \\alpha $\n",
    "\n",
    "- **Meaning**: Often used as a learning rate in optimization algorithms like gradient descent.\n",
    "- **Example**: $ \\alpha $ in $ x_{\\text{new}} = x_{\\text{old}} - \\alpha \\cdot \\nabla f(x) $\n",
    "\n",
    "### Beta $ \\beta $\n",
    "\n",
    "- **Meaning**: Commonly used as a decay rate or smoothing factor.\n",
    "- **Example**: $ \\beta $ in momentum-based gradient descent.\n",
    "\n",
    "### Gamma $ \\gamma $\n",
    "\n",
    "- **Meaning**: Used in various contexts, such as a discount factor in reinforcement learning.\n",
    "- **Example**: $ \\gamma $ in the discounted reward function.\n",
    "\n",
    "### Delta $ \\Delta $\n",
    "\n",
    "- **Meaning**: Represents change or difference in a quantity.\n",
    "- **Example**: $ \\Delta x $ to signify the change in $ x $.\n",
    "\n",
    "### Epsilon $ \\epsilon $\n",
    "\n",
    "- **Meaning**: Small positive constant, often used to avoid division by zero or to indicate a \"negligible\" quantity.\n",
    "- **Example**: $ \\epsilon $-greedy strategy in reinforcement learning.\n",
    "\n",
    "### Zeta $ \\zeta $\n",
    "\n",
    "- **Meaning**: Less common, but sometimes used in regularization terms.\n",
    "- **Example**: Riemann Zeta function in some mathematical optimizations.\n",
    "\n",
    "### Eta $ \\eta $\n",
    "\n",
    "- **Meaning**: Similar to alpha, used as a learning rate in some contexts.\n",
    "- **Example**: $ \\eta $ in AdaGrad and other adaptive learning rate methods.\n",
    "\n",
    "### Theta $ \\theta $\n",
    "\n",
    "- **Meaning**: General parameter vector in machine learning algorithms.\n",
    "- **Example**: $ \\theta $ in linear regression $ h_\\theta(x) = \\theta^T x $.\n",
    "\n",
    "### Lambda $ \\lambda $\n",
    "\n",
    "- **Meaning**: Regularization parameter.\n",
    "- **Example**: $ \\lambda $ in L1 or L2 regularization.\n",
    "\n",
    "### Mu $ \\mu $\n",
    "\n",
    "- **Meaning**: Represents the mean in statistics and machine learning.\n",
    "- **Example**: $ \\mu $ in Gaussian distribution.\n",
    "\n",
    "### Nu $ \\nu $\n",
    "\n",
    "- **Meaning**: Degrees of freedom in Support Vector Machines or other statistical measures.\n",
    "- **Example**: $ \\nu $-SVM.\n",
    "\n",
    "### Xi $ \\xi $\n",
    "\n",
    "- **Meaning**: Often used as a variable for input data or slack variables.\n",
    "- **Example**: $ \\xi $ in Support Vector Machines for non-linearly separable data.\n",
    "\n",
    "### Rho $ \\rho $\n",
    "\n",
    "- **Meaning**: Correlation coefficient or density in probability distributions.\n",
    "- **Example**: $ \\rho $ in Spearman's rank correlation.\n",
    "\n",
    "### Sigma $ \\sigma $\n",
    "\n",
    "- **Meaning**: Standard deviation or activation function in neural networks.\n",
    "- **Example**: $ \\sigma $ in Gaussian or sigmoid function.\n",
    "\n",
    "### Tau $ \\tau $\n",
    "\n",
    "- **Meaning**: Time constant in time series analysis or reinforcement learning.\n",
    "- **Example**: $ \\tau $ in temporal difference learning.\n",
    "\n",
    "### Phi $ \\phi $\n",
    "\n",
    "- **Meaning**: Feature map or basis functions in machine learning.\n",
    "- **Example**: $ \\phi(x) $ in kernel methods.\n",
    "\n",
    "### Chi $ \\chi $\n",
    "\n",
    "- **Meaning**: Less common, but sometimes used in statistical tests.\n",
    "- **Example**: $ \\chi^2 $ test.\n",
    "\n",
    "### Psi $ \\psi $\n",
    "\n",
    "- **Meaning**: Used in wavelet transformation and sometimes in neural networks.\n",
    "- **Example**: $ \\psi $ in wavelet transform.\n",
    "\n",
    "### Omega $ \\omega $\n",
    "\n",
    "- **Meaning**: Angular frequency in signal processing, sometimes used as a hyperparameter.\n",
    "- **Example**: $ \\omega $ in Fourier series.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
