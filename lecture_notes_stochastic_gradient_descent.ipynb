{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "### **ELI5 \n",
    "\n",
    "Imagine you're at the top of a hilly park and you have a ball. You want to roll the ball down to the lowest point of the park. Instead of looking at the whole park to find the quickest path down, you just look at the ground right in front of you. You give the ball a little push in the direction that seems to go downwards. You keep doing this, giving the ball little pushes, and watching which way it goes down. After several little pushes, the ball eventually reaches the lowest point.\n",
    "\n",
    "In this analogy:\n",
    "- The hilly park is like a function we want to minimize.\n",
    "- The ball is our current guess or solution.\n",
    "- The little pushes are small steps we take to adjust our solution to make it better.\n",
    "\n",
    "### **Or in another words:\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is an optimization algorithm commonly used in machine learning, especially for training deep neural networks.\n",
    "\n",
    "Let's break it down:\n",
    "\n",
    "1. **Gradient**: This refers to the slope or direction of the steepest increase of a function. For a multi-variable function, it's a vector of partial derivatives. In simpler terms, the gradient points in the direction where the function is increasing the fastest.\n",
    "2. **Descent**: This means we want to go in the opposite direction of the gradient. Why? Because we usually want to minimize a function, like the error in a machine learning model. By moving opposite to the gradient, we're heading towards the minimum.\n",
    "3. **Stochastic**: In the context of SGD, this means that instead of using the entire dataset to compute the gradient (which can be computationally expensive), we use just one or a small batch of data points. This introduces randomness (hence \"stochastic\"), as the direction of the gradient can vary based on which data points are chosen.\n",
    "\n",
    "The process is iterative:\n",
    "- Start with an initial guess for the solution (like initial weights in a neural network).\n",
    "- Randomly pick a data point (or a small batch) from the dataset.\n",
    "- Compute the gradient using that data point.\n",
    "- Update the solution by moving a small step in the opposite direction of the gradient.\n",
    "- Repeat until convergence or for a set number of iterations.\n",
    "\n",
    "The main advantage of SGD over traditional gradient descent is speed. Because it uses only a subset of the data at each step, it can make progress and adjust the solution without having to process the entire dataset. However, because of the stochastic nature, it can be noisier and might not always take the most direct path to the minimum, but with the right parameters, it can converge to a good solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic example in Python\n",
    "We'll try to find the minimum of the quadratic function f(x)=x**2 using Stochastic Gradient Descent (SGD).\n",
    "\n",
    "The true minimum of this function is at x=0\n",
    "\n",
    "Here's how the SGD process would look:\n",
    "\n",
    "* Initialization: Start with a random guess for x.\n",
    "* Gradient Calculation: f'(x) = 2x\n",
    "* Update Rule: Update x by subtracting a fraction (learning rate) of the gradient.\n",
    "* Iteration: Repeat the gradient calculation and update steps until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.7806166769441725e-05,\n",
       " [-2.64888617180133,\n",
       "  -2.119108937441064,\n",
       "  -1.6952871499528512,\n",
       "  -1.356229719962281,\n",
       "  -1.0849837759698249,\n",
       "  -0.8679870207758599,\n",
       "  -0.6943896166206879,\n",
       "  -0.5555116932965503,\n",
       "  -0.4444093546372402,\n",
       "  -0.35552748370979215,\n",
       "  -0.2844219869678337,\n",
       "  -0.22753758957426698,\n",
       "  -0.1820300716594136,\n",
       "  -0.14562405732753086,\n",
       "  -0.11649924586202469,\n",
       "  -0.09319939668961975,\n",
       "  -0.0745595173516958,\n",
       "  -0.05964761388135664,\n",
       "  -0.04771809110508531,\n",
       "  -0.03817447288406825,\n",
       "  -0.0305395783072546,\n",
       "  -0.02443166264580368,\n",
       "  -0.019545330116642945,\n",
       "  -0.015636264093314357,\n",
       "  -0.012509011274651486,\n",
       "  -0.01000720901972119,\n",
       "  -0.008005767215776952,\n",
       "  -0.006404613772621562,\n",
       "  -0.00512369101809725,\n",
       "  -0.0040989528144778,\n",
       "  -0.00327916225158224,\n",
       "  -0.0026233298012657918,\n",
       "  -0.0020986638410126334,\n",
       "  -0.0016789310728101067,\n",
       "  -0.0013431448582480853,\n",
       "  -0.0010745158865984683,\n",
       "  -0.0008596127092787747,\n",
       "  -0.0006876901674230197,\n",
       "  -0.0005501521339384157,\n",
       "  -0.0004401217071507326,\n",
       "  -0.0003520973657205861,\n",
       "  -0.0002816778925764689,\n",
       "  -0.0002253423140611751,\n",
       "  -0.0001802738512489401,\n",
       "  -0.00014421908099915208,\n",
       "  -0.00011537526479932167,\n",
       "  -9.230021183945733e-05,\n",
       "  -7.384016947156586e-05,\n",
       "  -5.907213557725269e-05,\n",
       "  -4.7257708461802155e-05,\n",
       "  -3.7806166769441725e-05])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function f(x) = x^2\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "# Derivative of f, f'(x) = 2x\n",
    "def df(x):\n",
    "    return 2*x\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "def sgd(initial_x, learning_rate, num_iterations):\n",
    "    x = initial_x\n",
    "    history = [x]\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        gradient = df(x)\n",
    "        x = x - learning_rate * gradient\n",
    "        history.append(x)\n",
    "        \n",
    "    return x, history\n",
    "\n",
    "# Parameters\n",
    "initial_x = np.random.uniform(-10, 10)  # Starting with a random guess between -10 and 10\n",
    "learning_rate = 0.1\n",
    "num_iterations = 50\n",
    "\n",
    "final_x, history = sgd(initial_x, learning_rate, num_iterations)\n",
    "\n",
    "final_x, history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro-ml-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
