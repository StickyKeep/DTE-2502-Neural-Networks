{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in AI\n",
    "\n",
    "## ELI5: Backpropagation\n",
    "\n",
    "Imagine you're trying to teach a robot to throw a ball into a hoop. Every time the robot misses, you slightly adjust the robot's arm and the force it uses. Over time, the robot gets better at throwing the ball into the hoop. Backpropagation in AI is similar to this. It's a way for the computer to learn from its mistakes and adjust itself to get better results.\n",
    "\n",
    "1. Start with random weights (like the robot's initial throw).\n",
    "2. See how far off the prediction is (the difference between the throw and the hoop).\n",
    "3. Adjust the weights a little bit to get closer to the correct answer.\n",
    "4. Repeat until the computer (or robot) is good at the task.\n",
    "\n",
    "## ELI Like More Advanced\n",
    "\n",
    "Backpropagation is a supervised learning algorithm used for training artificial neural networks. It's a method to calculate the gradient of the loss function concerning each weight by the chain rule.\n",
    "\n",
    "### Steps of Backpropagation:\n",
    "\n",
    "1. **Forward Pass**:\n",
    "    * Input a training sample.\n",
    "    * Pass it through the network to get the prediction.\n",
    "    * Calculate the error (difference between predicted and actual value).\n",
    "\n",
    "2. **Backward Pass**:\n",
    "    * Calculate the gradient of the error concerning each weight. This tells us how much the error would change if we changed the weights by a tiny amount.\n",
    "    * Update the weights in the network using a learning rate.\n",
    "\n",
    "### Important Formulas:\n",
    "\n",
    "* The error for each neuron is calculated as:\n",
    "    $$ \\delta_j = (y_j - a_j) \\cdot f'(h_j) $$\n",
    "\n",
    "    where $ \\delta_j $ is the error term for neuron $ j $, $ y_j $ is the actual output, $ a_j $ is the predicted output, and $ f'(h_j) $ is the derivative of the activation function for neuron $ j $.\n",
    "\n",
    "* The weights are updated using the formula:\n",
    "    $$ w_{ij} \\leftarrow w_{ij} - \\mu \\cdot \\delta_j \\cdot x_i $$\n",
    "\n",
    "    where $ w_{ij} $ is the weight from neuron $ i $ to neuron $ j $, $ \\mu $) is the learning rate, $ \\delta_j $ is the error term for neuron $ j $, and $ x_i $ is the output of neuron $ i $.\n",
    "\n",
    "In simpler terms, backpropagation helps the network learn from its mistakes by adjusting the weights in the direction that reduces the error. It's like tweaking the knobs and dials of a complex machine until it works just right.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)\n",
    "\n",
    "A Multilayer Perceptron (MLP) is a class of feedforward artificial neural network. It consists of at least three layers of nodes: an input layer, one or more hidden layers, and an output layer. Each node (or neuron) in a layer is connected to every node in the subsequent layer, with each connection having an associated weight.\n",
    "\n",
    "## Architecture:\n",
    "\n",
    "1. **Input Layer**: \n",
    "    * The initial layer where data is fed into the network.\n",
    "    * It has as many nodes as there are input features.\n",
    "\n",
    "2. **Hidden Layers**: \n",
    "    * These layers are between the input and output layers.\n",
    "    * Neurons in hidden layers process patterns in the data by weighing inputs.\n",
    "\n",
    "3. **Output Layer**: \n",
    "    * The final layer that produces the prediction or classification result.\n",
    "    * It has as many neurons as there are classes for classification tasks or just one neuron for regression tasks.\n",
    "\n",
    "## Activation Functions:\n",
    "Neurons typically have an activation function that transforms their weighted input into an output signal. Common activation functions include:\n",
    "* Sigmoid\n",
    "* ReLU (Rectified Linear Unit)\n",
    "* Tanh (Hyperbolic Tangent)\n",
    "\n",
    "## Basic Python Illustration:\n",
    "\n",
    "Let's consider a simple MLP with:\n",
    "* 3 input features.\n",
    "* 1 hidden layer with 4 neurons.\n",
    "* 1 output neuron (for a binary classification task).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16324619]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions and their derivatives\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid Activation Function:\n",
    "    \n",
    "    This function squashes values between 0 and 1. It's widely used for \n",
    "    outputs of binary classification problems.\n",
    "    \n",
    "    Args:\n",
    "    - x (float): Input value or array.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Transformed value between 0 and 1.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of the Sigmoid Function:\n",
    "    \n",
    "    This function returns the gradient of the sigmoid function at a given point.\n",
    "    It's used during backpropagation to adjust weights based on the error.\n",
    "    \n",
    "    Args:\n",
    "    - x (float): Input value or array.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Gradient of the sigmoid function at 'x'.\n",
    "    \"\"\"\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Sample MLP\n",
    "\n",
    "class SimpleMLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"Initializer for the SimpleMLP class.\n",
    "        \n",
    "        Initializes weights and biases for the network. Weights are \n",
    "        initialized with random values, which will be adjusted during \n",
    "        training.\n",
    "        \n",
    "        Args:\n",
    "        - input_size (int): Number of nodes in the input layer.\n",
    "        - hidden_size (int): Number of nodes in the hidden layer.\n",
    "        - output_size (int): Number of nodes in the output layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize weights and biases for input to hidden layer connections\n",
    "        # Weights are matrices where each entry i,j is the weight from the i-th input node to the j-th hidden node.\n",
    "\n",
    "        self.weights_input_to_hidden = np.random.randn(input_size, hidden_size)\n",
    "\n",
    "        # Biases are added to the inputs to introduce non-linearity and flexibility to the model.\n",
    "\n",
    "        self.bias_hidden = np.random.randn(hidden_size)\n",
    "        \n",
    "        # Initialize weights and biases for hidden to output layer connections\n",
    "        self.weights_hidden_to_output = np.random.randn(hidden_size, output_size)\n",
    "        self.bias_output = np.random.randn(output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward Pass Through the Network.\n",
    "        \n",
    "        Takes an input 'x' and passes it through the network to produce an output.\n",
    "        \n",
    "        Args:\n",
    "        - x (array): Input data.\n",
    "        \n",
    "        Returns:\n",
    "        - array: Output from the network.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Input to Hidden Layer\n",
    "        # Calculate the dot product of the input data and weights, then add the bias\n",
    "\n",
    "        self.hidden_input = np.dot(x, self.weights_input_to_hidden) + self.bias_hidden\n",
    "\n",
    "        # Apply the sigmoid activation function to introduce non-linearity\n",
    "\n",
    "        self.hidden_output = sigmoid(self.hidden_input)\n",
    "        \n",
    "        # Hidden to Output Layer\n",
    "        # Calculate the dot product of the hidden layer output and weights, then add the bias\n",
    "\n",
    "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_to_output) + self.bias_output\n",
    "\n",
    "        # Apply the sigmoid activation function to get the final output\n",
    "        \n",
    "        self.final_output = sigmoid(self.output_input)\n",
    "        \n",
    "        return self.final_output\n",
    "\n",
    "# Create an instance of the MLP and test with a sample input\n",
    "mlp = SimpleMLP(input_size=3, hidden_size=4, output_size=1)\n",
    "sample_input = np.array([0.5, 0.6, 0.7])\n",
    "output = mlp.forward(sample_input)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Stochastic Gradient Descent is a popular optimization algorithm used to minimize (or maximize) a function iteratively. It's especially useful for training large-scale machine learning models. Let's break down the name into its components to understand its meaning and working:\n",
    "\n",
    "## 1. Gradient Descent:\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize (or maximize) functions. The basic idea behind gradient descent is to:\n",
    "\n",
    "1. Calculate the gradient (or slope) of the function at a given point.\n",
    "2. Move in the opposite direction of the gradient (for minimization) by a certain step size or \"learning rate.\"\n",
    "3. Repeat the process until the function reaches a minimum (or maximum) value.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "* **Gradient**: It is the vector of partial derivatives. In the context of machine learning, the function we want to minimize is usually the loss or error function, and the gradient gives the direction of steepest increase of this function.\n",
    "* **Learning Rate**: This is a hyperparameter that determines the size of the steps we take in the direction opposite to the gradient. A high learning rate can lead to overshooting the minimum, while a low learning rate can make the convergence very slow.\n",
    "\n",
    "## 2. Stochastic:\n",
    "\n",
    "In classical (or \"batch\") gradient descent, the gradient is computed using the entire dataset, which can be computationally expensive and time-consuming for large datasets. \"Stochastic\" in SGD refers to the use of a single random data point (or a small subset called a mini-batch) from the dataset to compute the gradient at each step instead of the entire dataset.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "* **Stochasticity**: Introduces randomness into the algorithm, which can help escape local minima and converge faster than batch gradient descent. However, this also means that SGD can have a lot of variance and might jump around the optimal value.\n",
    "* **Mini-batch SGD**: A compromise between batch and pure stochastic gradient descent. Instead of using the entire dataset or a single example, a mini-batch of samples is used to compute the gradient. This can offer a balance between computational efficiency and convergence stability.\n",
    "\n",
    "## Summary:\n",
    "\n",
    "Stochastic Gradient Descent is an optimization technique where the model parameters are updated using the gradient of the error with respect to a single (or a mini-batch of) training example(s) rather than the entire training dataset. This approach can be much faster and can also navigate out of local minima, but might be noisier than the traditional gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weight after SGD: -1.4341845338324233\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a simple quadratic loss function: L(w) = (w - 3)^2\n",
    "# The minimum value of this function is 0, achieved when w = 3.\n",
    "def loss_function(w):\n",
    "    \"\"\"Quadratic Loss Function\n",
    "    \n",
    "    This function calculates the loss for given weight 'w'.\n",
    "    It's a simple quadratic function centered around 3.\n",
    "    \n",
    "    Args:\n",
    "    - w (float): Current weight value.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Loss value.\n",
    "    \"\"\"\n",
    "    return (w - 3) ** 2\n",
    "\n",
    "def gradient(w):\n",
    "    \"\"\"Gradient of the Quadratic Loss Function\n",
    "    \n",
    "    This function calculates the gradient of the loss with respect to 'w'.\n",
    "    For the function L(w) = (w - 3)^2, its gradient is dL/dw = 2(w - 3).\n",
    "    \n",
    "    Args:\n",
    "    - w (float): Current weight value.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Gradient value.\n",
    "    \"\"\"\n",
    "    return 2 * (w - 3)\n",
    "\n",
    "def stochastic_gradient_descent(epochs, learning_rate, data):\n",
    "    \"\"\"Stochastic Gradient Descent Optimization\n",
    "    \n",
    "    This function attempts to find the weight 'w' that minimizes the loss function \n",
    "    using stochastic gradient descent.\n",
    "    \n",
    "    Args:\n",
    "    - epochs (int): Number of passes through the dataset.\n",
    "    - learning_rate (float): Step size for each weight update.\n",
    "    - data (list): Dataset containing weight samples.\n",
    "    \n",
    "    Returns:\n",
    "    - list: History of weight values throughout the training.\n",
    "    \"\"\"\n",
    "    # Initialize weight randomly\n",
    "    w = np.random.randn()\n",
    "    weight_history = [w]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle data to ensure randomness in picking samples\n",
    "        np.random.shuffle(data)\n",
    "        for sample in data:\n",
    "            # Calculate gradient using a single data point (stochasticity)\n",
    "            grad = gradient(sample)\n",
    "            # Update weight in the direction of negative gradient\n",
    "            w -= learning_rate * grad\n",
    "            weight_history.append(w)\n",
    "    \n",
    "    return weight_history\n",
    "\n",
    "# Test our SGD implementation\n",
    "data_samples = np.linspace(0, 6, 100)  # Generate 100 samples between 0 and 6\n",
    "epochs = 10\n",
    "learning_rate = 0.1\n",
    "weights = stochastic_gradient_descent(epochs, learning_rate, data_samples)\n",
    "\n",
    "# Print final weight\n",
    "print(\"Final weight after SGD:\", weights[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation: Forward and Backward Phases\n",
    "\n",
    "Backpropagation is the backbone of training deep neural networks. It consists of two main phases:\n",
    "\n",
    "## 1. Forward Phase\n",
    "\n",
    "In this phase, we move from the input layer to the output layer, predicting the output:\n",
    "\n",
    "1. **Input Data**: Begin by feeding a training example into the network.\n",
    "2. **Calculate Activations**: For each layer:\n",
    "    * Compute the weighted sum of the inputs from the previous layer.\n",
    "    * Apply an activation function (like sigmoid or ReLU) to these weighted sums.\n",
    "    * This produces the activation values for the current layer.\n",
    "3. **Predict Output**: After passing through all layers, the network produces a prediction based on the final layer's activations.\n",
    "\n",
    "The forward phase's main goal is to compute the network's output and see how it compares to the actual target or label.\n",
    "\n",
    "## 2. Backward Phase\n",
    "\n",
    "This phase is where the learning happens. We move from the output layer back to the input layer, adjusting the weights:\n",
    "\n",
    "1. **Compute Error**: Calculate the difference between the predicted output and the actual label. This tells us how well (or poorly) our network performed.\n",
    "2. **Propagate Error Backward**: For each layer, starting from the output:\n",
    "    * Compute the gradient of the loss with respect to the activations.\n",
    "    * Use this gradient to calculate how much each neuron in the previous layer contributed to the error (this is the \"backpropagation\" step).\n",
    "3. **Update Weights**: Adjust the weights in each layer using the gradients computed in the previous step. The adjustments are made in the direction that reduces the error.\n",
    "\n",
    "The backward phase's main goal is to find out how much each weight in the network contributed to the error and then adjust it to reduce the error.\n",
    "\n",
    "## Summary:\n",
    "\n",
    "Think of backpropagation as a teacher correcting a student's homework. \n",
    "\n",
    "* In the **forward phase**, the student (neural network) tries to solve the problem (makes a prediction). \n",
    "* In the **backward phase**, the teacher (backpropagation algorithm) checks the work, sees where the student made mistakes, and provides guidance on how to correct them (adjusts the weights).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudocode for Backpropagation Algorithm\n",
    "\n",
    "## 1. Initialization:\n",
    "- Initialize all weights and biases in the network randomly.\n",
    "- Define a learning rate $ \\alpha $.\n",
    "\n",
    "## 2. For each training sample:\n",
    "### Forward Phase:\n",
    "\n",
    "1. **Input Data**: \n",
    "    - Set the activations for the input layer: $ a^{(0)} = \\text{input} $\n",
    "\n",
    "2. **Propagation Forward through the Layers**:\n",
    "    - For layer $ l = 1 $ to $ L $ (where $ L $ is the output layer):\n",
    "        - Compute weighted sum: $ z^{(l)} = w^{(l)} \\cdot a^{(l-1)} + b^{(l)} $\n",
    "        - Compute activation: $ a^{(l)} = \\text{activation\\_function}(z^{(l)}) $\n",
    "\n",
    "3. **Output**: \n",
    "    - The prediction will be the activations from the last layer: $ \\text{prediction} = a^{(L)} $\n",
    "\n",
    "### Backward Phase:\n",
    "\n",
    "4. **Compute Output Layer Error**:\n",
    "    - Calculate the difference between the network's output and the actual target: $ \\delta^{(L)} = \\text{prediction} - \\text{target} $\n",
    "\n",
    "5. **Backpropagate the Error**:\n",
    "    - For layer $ l = L-1 $ down to $ 1 $:\n",
    "        - Compute the gradient of the activation function: $ g' = \\text{activation\\_function\\_derivative}(z^{(l)}) $\n",
    "        - Compute the error for layer $ l $: $ \\delta^{(l)} = (w^{(l+1)})^T \\cdot \\delta^{(l+1)} \\times g' $\n",
    "\n",
    "6. **Update Weights and Biases**:\n",
    "    - For layer $ l = 1 $ to $ L $:\n",
    "        - Update weights: $ w^{(l)} = w^{(l)} - \\alpha \\cdot \\delta^{(l)} \\cdot (a^{(l-1)})^T $\n",
    "        - Update biases: $ b^{(l)} = b^{(l)} - \\alpha \\cdot \\delta^{(l)} $\n",
    "\n",
    "## 3. Iterate:\n",
    "- Repeat the process for a specified number of iterations or until the network converges (i.e., the error becomes sufficiently small).\n",
    "\n",
    "## 4. End:\n",
    "- After training, use the optimized weights and biases for predictions on new, unseen data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro-ml-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
